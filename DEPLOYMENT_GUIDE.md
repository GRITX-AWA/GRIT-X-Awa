# üöÄ Deployment Guide - Frontend & Backend

## üìã Current Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ  USER BROWSER                                              ‚îÇ
‚îÇ  ‚îî‚îÄ> Frontend (Vercel - Static Site)                      ‚îÇ
‚îÇ       ‚îî‚îÄ> Calls API_BASE_URL                              ‚îÇ
‚îÇ            ‚îî‚îÄ> Backend (Google Cloud Run)                 ‚îÇ
‚îÇ                 ‚îú‚îÄ> FastAPI Server                        ‚îÇ
‚îÇ                 ‚îú‚îÄ> ML Models (~28MB Kepler + TESS)      ‚îÇ
‚îÇ                 ‚îú‚îÄ> Supabase Integration                  ‚îÇ
‚îÇ                 ‚îî‚îÄ> Returns Predictions                   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚ö†Ô∏è **IMPORTANT: Vercel Limitations for ML Backend**

### **Why the Backend CAN'T Run on Vercel:**

| Issue | Vercel Serverless | Google Cloud Run | Impact |
|-------|------------------|------------------|---------|
| **Model Size** | 50MB limit | 10GB container limit | Kepler+TESS = ~56MB total |
| **Memory** | 1GB max | 2GB+ configurable | ML needs 2GB+ |
| **Execution Time** | 10-60 seconds | 300 seconds (5 min) | Large files need >2 min |
| **Cold Starts** | Every request | Minimized with min instances | Models load slowly |
| **Model Preloading** | Not possible | ‚úÖ Loads on startup | Critical for performance |

### **Your ML Models:**
```bash
Kepler Models:  ~28MB total
  - cat_model.pkl:   6.4MB (CatBoost)
  - xgb_model.pkl:   6.4MB (XGBoost)
  - lgbm_model.pkl: 13.0MB (LightGBM)
  - imputer.pkl:     1.8MB (Preprocessing)

TESS Models:    ~28MB total
  - Similar structure

TOTAL:          ~56MB (exceeds Vercel 50MB limit)
```

### **Vercel Serverless Functions:**
- ‚ùå 50MB deployment size limit (you have 56MB)
- ‚ùå 10-60 second timeout (you need 5 minutes)
- ‚ùå 1GB memory max (ML needs 2GB+)
- ‚ùå No persistent model caching
- ‚ùå Cold start every request = slow

### **Google Cloud Run (Current):**
- ‚úÖ 10GB container size (plenty of room)
- ‚úÖ 5 minute timeout (perfect for large files)
- ‚úÖ 2GB+ memory (configurable)
- ‚úÖ Models preloaded at startup
- ‚úÖ Fast predictions after warmup

---

## ‚úÖ **Recommended Deployment (Current Setup)**

### **1. Frontend ‚Üí Vercel**
**What:** Static Vite/React app
**Why:** Fast global CDN, free hosting, perfect for SPAs
**Size:** ~5-10MB

**Deploy:**
```bash
cd frontend
npm run build
vercel --prod
```

**Environment Variables (Vercel Dashboard):**
```env
VITE_API_URL=https://grit-x-awa-1035421252747.europe-west1.run.app
PUBLIC_SUPABASE_URL=https://nafpqdeyshrdstecqldc.supabase.co
PUBLIC_SUPABASE_ANON_KEY=your-anon-key
```

---

### **2. Backend ‚Üí Google Cloud Run**
**What:** FastAPI + ML models in Docker container
**Why:** Handles large ML models, long processing times
**Size:** ~500MB Docker image

**Deploy:**
```bash
cd backend
./deploy.sh
# Or
./deploy.ps1
```

**Already Deployed At:**
```
https://grit-x-awa-1035421252747.europe-west1.run.app
```

**Configuration:**
- Memory: 2GB
- CPU: 2 cores
- Timeout: 300 seconds (5 minutes)
- Min instances: 0 (scales to zero)
- Max instances: 10

---

## üîß **Alternative: Backend on Vercel (NOT RECOMMENDED)**

If you **really** want to try Vercel for backend, you'd need to:

### **Option A: Model Splitting (Complex)**
```bash
# Split models across multiple serverless functions
/api/predict-kepler  -> Loads only Kepler models
/api/predict-tess    -> Loads only TESS models
```

**Limitations:**
- Each function still limited to 50MB
- Still limited to 10-60 seconds (will timeout on large files)
- Cold starts on every request
- Much slower than Cloud Run

### **Option B: External Model Hosting**
```bash
# Store models in external storage
- Upload models to S3/Google Cloud Storage
- Download at runtime (adds 10-30s latency)
- Still limited by timeout
```

**Limitations:**
- Very slow first request
- Still timeout issues
- More complex architecture

---

## üéØ **Best Practice: Hybrid Deployment (CURRENT)**

### **Frontend: Vercel**
‚úÖ Fast global CDN
‚úÖ Free tier sufficient
‚úÖ Automatic HTTPS
‚úÖ Easy deployment from Git

### **Backend: Google Cloud Run**
‚úÖ Handles ML workloads
‚úÖ 5-minute timeout
‚úÖ Model preloading
‚úÖ Auto-scaling
‚úÖ Pay only for usage

### **Database: Supabase**
‚úÖ PostgreSQL database
‚úÖ File storage (CSV uploads)
‚úÖ Real-time subscriptions
‚úÖ Free tier sufficient

---

## üìù **Deployment Checklist**

### **Frontend (Vercel)**
- [ ] Build succeeds: `npm run build`
- [ ] Set environment variable: `VITE_API_URL`
- [ ] Deploy: `vercel --prod`
- [ ] Test: Open Vercel URL
- [ ] Verify: Check Network tab ‚Üí calls to Cloud Run URL

### **Backend (Google Cloud Run)**
- [ ] Docker builds: `docker build -t test .`
- [ ] Set environment variables in Cloud Run
  - [ ] `SUPABASE_URL`
  - [ ] `SUPABASE_SERVICE_ROLE_KEY`
  - [ ] `SUPABASE_ANON_KEY`
- [ ] Deploy: `./deploy.sh` or `./deploy.ps1`
- [ ] Health check: `./health-check.ps1`
- [ ] Test: Visit `/api/v1/docs`

### **Integration Test**
- [ ] Upload CSV via frontend
- [ ] Check Network tab for API calls
- [ ] Verify predictions display
- [ ] Check Supabase for saved data

---

## üö® **Common Issues**

### **"Failed to fetch" Error**
**Cause:** Frontend can't reach backend
**Fix:** Check `VITE_API_URL` in Vercel environment variables

### **"Request Timeout"**
**Cause:** Processing takes >5 minutes
**Fix:**
1. Split large files into smaller batches
2. Increase Cloud Run timeout (already at max 5 min)
3. Implement async job processing

### **Cold Start Slow**
**Cause:** First request loads models
**Fix:** Set Cloud Run min instances to 1 (costs ~$20/month)

### **CORS Errors**
**Cause:** Backend not allowing frontend origin
**Fix:** Backend already allows all origins (`allow_origins=["*"]`)

---

## üí∞ **Cost Estimates (Monthly)**

### **Current Setup:**
```
Frontend (Vercel):        $0     (Free tier)
Backend (Cloud Run):      $5-15  (Pay per request)
Database (Supabase):      $0     (Free tier)
----------------------------------------
TOTAL:                    $5-15/month
```

### **If Using Min Instances = 1 (Always warm):**
```
Frontend (Vercel):        $0
Backend (Cloud Run):      $20-30  (Always running)
Database (Supabase):      $0
----------------------------------------
TOTAL:                    $20-30/month
```

---

## üìä **Performance Comparison**

| Metric | Vercel Serverless | Cloud Run (Current) |
|--------|------------------|---------------------|
| **First Request** | 15-30s (cold start) | 2-5s (preloaded) |
| **Subsequent** | 10-20s (reload models) | 0.5-2s (cached) |
| **Large File (9564 rows)** | ‚ùå Timeout | ‚úÖ 2-4 minutes |
| **Max Timeout** | 60s | 300s |
| **Memory Available** | 1GB | 2GB+ |

---

## ‚úÖ **Conclusion: Keep Current Setup**

**Your current deployment is optimal:**

1. ‚úÖ Frontend on Vercel - Fast, free, global CDN
2. ‚úÖ Backend on Cloud Run - Handles ML workloads perfectly
3. ‚úÖ Models preloaded - Fast predictions
4. ‚úÖ 5-minute timeout - Handles large files
5. ‚úÖ Auto-scaling - Pay only for usage

**Don't migrate backend to Vercel because:**
- ‚ùå Model size exceeds Vercel limits
- ‚ùå Timeout too short for ML processing
- ‚ùå No model preloading = slow every request
- ‚ùå More expensive for ML workloads

---

## üîó **Useful Links**

- **Frontend:** https://grit-x-awa.vercel.app (deploy here)
- **Backend:** https://grit-x-awa-1035421252747.europe-west1.run.app (keep here)
- **Backend API Docs:** https://grit-x-awa-1035421252747.europe-west1.run.app/api/v1/docs
- **Cloud Run Console:** https://console.cloud.google.com/run/detail/europe-west1/grit-x-awa
- **Vercel Dashboard:** https://vercel.com/dashboard

---

## üÜò **Need Help?**

1. Check [TROUBLESHOOTING.md](./TROUBLESHOOTING.md)
2. Run backend health check: `./backend/health-check.ps1`
3. Check Cloud Run logs: `gcloud run services logs tail grit-x-awa --region europe-west1`
4. Test API directly: Visit `/api/v1/docs` endpoint
